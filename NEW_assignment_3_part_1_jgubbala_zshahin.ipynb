{
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "view-in-github",
        "colab_type": "text"
      },
      "source": [
        "<a href=\"https://colab.research.google.com/github/jahnavigubbala/Assignment-3-RL/blob/main/NEW_assignment_3_part_1_jgubbala_zshahin.ipynb\" target=\"_parent\"><img src=\"https://colab.research.google.com/assets/colab-badge.svg\" alt=\"Open In Colab\"/></a>"
      ]
    },
    {
      "metadata": {
        "id": "cf4d8c9b006ada45"
      },
      "cell_type": "markdown",
      "source": [
        "## <center>CSE 546: Reinforcement Learning</center>\n",
        "### <center>Prof. Alina Vereshchaka</center>\n",
        "#### <center>Fall 2025</center>\n",
        "\n",
        "Welcome to the Assignment 3, Part 1: Introduction to Actor-Critic Methods! It includes the implementation of simple actor and critic networks and best practices used in modern Actor-Critic algorithms."
      ],
      "id": "cf4d8c9b006ada45"
    },
    {
      "metadata": {
        "id": "9d7a6d891e2fb312"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 0: Setup and Imports"
      ],
      "id": "9d7a6d891e2fb312"
    },
    {
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "53473293aa9daf8e",
        "outputId": "2dd3f75b-5364-4d81-bb5b-23ffafaf4832"
      },
      "cell_type": "code",
      "source": [
        "!pip install ale-py -q\n",
        "!pip install shimmy -q\n",
        "!pip install \"gymnasium[atari]\" -q\n",
        "!pip install \"gymnasium[accept-rom-license]\" -q\n",
        "!apt-get update -y\n",
        "!apt-get install -y swig\n",
        "!pip install \"gymnasium[box2d]\" box2d box2d-py\n",
        "\n",
        "import numpy as np\n",
        "import torch\n",
        "import torch.nn as nn\n",
        "import torch.nn.functional as F\n",
        "import torch.optim as optim\n",
        "import gymnasium as gym\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import deque\n",
        "import ale_py\n",
        "import shimmy\n",
        "from gym.wrappers import AtariPreprocessing, FrameStack\n",
        "\n",
        "# Set seed for reproducibility\n",
        "SEED = 42\n",
        "np.random.seed(SEED)\n",
        "torch.manual_seed(SEED)"
      ],
      "id": "53473293aa9daf8e",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\u001b[33mWARNING: gymnasium 1.2.2 does not provide the extra 'accept-rom-license'\u001b[0m\u001b[33m\n",
            "Hit:1 https://developer.download.nvidia.com/compute/cuda/repos/ubuntu2204/x86_64  InRelease\n",
            "Hit:2 http://archive.ubuntu.com/ubuntu jammy InRelease\n",
            "Hit:3 http://security.ubuntu.com/ubuntu jammy-security InRelease\n",
            "Hit:4 http://archive.ubuntu.com/ubuntu jammy-updates InRelease\n",
            "Hit:5 https://cloud.r-project.org/bin/linux/ubuntu jammy-cran40/ InRelease\n",
            "Hit:6 http://archive.ubuntu.com/ubuntu jammy-backports InRelease\n",
            "Hit:7 https://cli.github.com/packages stable InRelease\n",
            "Hit:8 https://r2u.stat.illinois.edu/ubuntu jammy InRelease\n",
            "Hit:9 https://ppa.launchpadcontent.net/deadsnakes/ppa/ubuntu jammy InRelease\n",
            "Hit:10 https://ppa.launchpadcontent.net/graphics-drivers/ppa/ubuntu jammy InRelease\n",
            "Hit:11 https://ppa.launchpadcontent.net/ubuntugis/ppa/ubuntu jammy InRelease\n",
            "Reading package lists... Done\n",
            "W: Skipping acquire of configured file 'main/source/Sources' as repository 'https://r2u.stat.illinois.edu/ubuntu jammy InRelease' does not seem to provide it (sources.list entry misspelt?)\n",
            "Reading package lists... Done\n",
            "Building dependency tree... Done\n",
            "Reading state information... Done\n",
            "swig is already the newest version (4.0.2-1ubuntu1).\n",
            "0 upgraded, 0 newly installed, 0 to remove and 43 not upgraded.\n",
            "Requirement already satisfied: box2d in /usr/local/lib/python3.12/dist-packages (2.3.10)\n",
            "Requirement already satisfied: box2d-py in /usr/local/lib/python3.12/dist-packages (2.3.5)\n",
            "Requirement already satisfied: gymnasium[box2d] in /usr/local/lib/python3.12/dist-packages (1.2.2)\n",
            "Requirement already satisfied: numpy>=1.21.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.0.2)\n",
            "Requirement already satisfied: cloudpickle>=1.2.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (3.1.2)\n",
            "Requirement already satisfied: typing-extensions>=4.3.0 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.15.0)\n",
            "Requirement already satisfied: farama-notifications>=0.0.1 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (0.0.4)\n",
            "Requirement already satisfied: pygame>=2.1.3 in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (2.6.1)\n",
            "Requirement already satisfied: swig==4.* in /usr/local/lib/python3.12/dist-packages (from gymnasium[box2d]) (4.4.0)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "Gym has been unmaintained since 2022 and does not support NumPy 2.0 amongst other critical functionality.\n",
            "Please upgrade to Gymnasium, the maintained drop-in replacement of Gym, or contact the authors of your software and request that they upgrade.\n",
            "See the migration guide at https://gymnasium.farama.org/introduction/migration_guide/ for additional information.\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<torch._C.Generator at 0x790273d13550>"
            ]
          },
          "metadata": {},
          "execution_count": 3
        }
      ],
      "execution_count": 3
    },
    {
      "metadata": {
        "id": "2a3d9c34ff222994"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 1: Actor-Critic Network Architectures and Loss Computation\n",
        "\n",
        "In this section, you will explore two common architectural designs for Actor-Critic methods and implement their corresponding loss functions using dummy tensors. These architectures are:\n",
        "- A. Completely separate actor and critic networks\n",
        "- B. A shared network with two output heads\n",
        "\n",
        "Both designs are widely used in practice. Shared networks are often more efficient and generalize better, while separate networks offer more control and flexibility.\n",
        "\n",
        "---\n"
      ],
      "id": "2a3d9c34ff222994"
    },
    {
      "metadata": {
        "id": "971fa7887dd4f858"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1a â€“ Separate Actor and Critic Networks with Loss Function\n",
        "\n",
        "Define a class `SeparateActorCritic`. Your goal is to:\n",
        "- Create two completely independent neural networks: one for the actor and one for the critic.\n",
        "- The actor should output a probability distribution over discrete actions (use `nn.Softmax`).\n",
        "- The critic should output a single scalar value.\n",
        "\n",
        " Use `nn.ReLU()` as your activation function. Include at least one hidden layer of reasonable width (e.g. 64 or 128 units).\n",
        "\n",
        "```python\n",
        "# TODO: Define SeparateActorCritic class\n",
        "```\n",
        "\n",
        " Next, simulate training using dummy tensors:\n",
        "1. Generate dummy tensors for log-probabilities, returns, estimated values, and entropies.\n",
        "2. Compute the actor loss using the advantage (return - value).\n",
        "3. Compute the critic loss as mean squared error between values and returns.\n",
        "4. Use a single optimizer for both the Actor and the Critic. In this case, combine the actor and critic losses into a total loss and perform backpropagation.\n",
        "5. Use a separate optimizers for both the Actor and the Critic. In this case, keep the actor and critic losses separate and perform backpropagation.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate loss computation and backpropagation\n",
        "```\n",
        "\n",
        "ðŸ”— Helpful references:\n",
        "- PyTorch Softmax: https://pytorch.org/docs/stable/generated/torch.nn.Softmax.html\n",
        "- PyTorch MSE Loss: https://pytorch.org/docs/stable/generated/torch.nn.functional.mse_loss.html\n",
        "\n",
        "---"
      ],
      "id": "971fa7887dd4f858"
    },
    {
      "metadata": {
        "id": "dd6b81ed1791e4e6"
      },
      "cell_type": "code",
      "source": [
        "# TODO: Define a class SeparateActorCritic with separate networks for actor and critic\n",
        "\n",
        "# BEGIN_YOUR_CODE\n",
        "class SeparateActorCritic(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_size = 128):\n",
        "    super(SeparateActorCritic, self).__init__()\n",
        "\n",
        "    # Actor\n",
        "    self.actor_fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.actor_fc2 = nn.Linear(hidden_size, output_size)\n",
        "\n",
        "    # Critic\n",
        "    self.critic_fc1 = nn.Linear(input_size, hidden_size)\n",
        "    self.critic_fc2 = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    actor = F.relu(self.actor_fc1(x))\n",
        "    actor_out = self.softmax(self.actor_fc2(actor))\n",
        "\n",
        "    critic = F.relu(self.critic_fc1(x))\n",
        "    critic_out = self.critic_fc2(critic)\n",
        "\n",
        "    return actor_out, critic_out\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "dd6b81ed1791e4e6",
      "outputs": [],
      "execution_count": 4
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Simulate training using dummy tensors"
      ],
      "metadata": {
        "id": "6eauU5RAMRcK"
      },
      "id": "6eauU5RAMRcK"
    },
    {
      "cell_type": "code",
      "source": [
        "log_prob = torch.randn(7, requires_grad=True)\n",
        "returns = torch.randn(7)\n",
        "values = torch.randn(7, requires_grad=True)\n",
        "entropy = torch.rand(1)\n",
        "\n",
        "# Advantage\n",
        "advantage = returns - values.detach()\n",
        "\n",
        "# Loss\n",
        "actor_loss = -(log_prob * advantage).mean()\n",
        "critic_loss = F.mse_loss(values, returns)"
      ],
      "metadata": {
        "id": "NR0Kd4WJCcig"
      },
      "id": "NR0Kd4WJCcig",
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Single optimizer for both actor and critic"
      ],
      "metadata": {
        "id": "4oIvaEFAMWmU"
      },
      "id": "4oIvaEFAMWmU"
    },
    {
      "cell_type": "code",
      "source": [
        "total_loss = actor_loss + critic_loss\n",
        "optimizer = optim.Adam([log_prob, values], lr=0.001)\n",
        "optimizer.zero_grad()\n",
        "total_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(f\"Total loss for the single optimizer:  {total_loss.item():.3f}\")"
      ],
      "metadata": {
        "id": "qeE6WGesIy1Y",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "a2191890-4fbd-4c55-b721-1b2879adeaec"
      },
      "id": "qeE6WGesIy1Y",
      "execution_count": 6,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Total loss for the single optimizer:  3.693\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [
        "#### Seperater optimizer for  actor and critic"
      ],
      "metadata": {
        "id": "wtcrtfw2Mbvr"
      },
      "id": "wtcrtfw2Mbvr"
    },
    {
      "cell_type": "code",
      "source": [
        "actor_loss = -(log_prob * advantage).mean()\n",
        "critic_loss = F.mse_loss(values, returns)\n",
        "\n",
        "# Separate optimizers\n",
        "actor_optimizer = optim.Adam([log_prob], lr=0.001)\n",
        "critic_optimizer = optim.Adam([values], lr=0.001)\n",
        "\n",
        "actor_optimizer.zero_grad()\n",
        "actor_loss.backward()\n",
        "actor_optimizer.step()\n",
        "\n",
        "critic_optimizer.zero_grad()\n",
        "critic_loss.backward()\n",
        "critic_optimizer.step()\n",
        "\n",
        "print(f\"Actor loss:  {actor_loss.item():.3f}\")\n",
        "print(f\"Critic loss: {critic_loss.item():.3f}\")"
      ],
      "metadata": {
        "id": "D_CpZztfIz-T",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf33aa33-4772-4269-be6f-efef861a1b4f"
      },
      "id": "D_CpZztfIz-T",
      "execution_count": 7,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actor loss:  1.298\n",
            "Critic loss: 2.392\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "eb8e90c88108cd2e"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "\n",
        "Sharing some layers can have many advantages that are discussed below after the implementation of the `SharedActorCritic`. However, the completely seperate architecture can make learning more stable, as the actor and critic gradients are isolated from each other.\n",
        "\n"
      ],
      "id": "eb8e90c88108cd2e"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "HLBNByUYy27o"
      },
      "id": "HLBNByUYy27o",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "64081a606b93029d"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 1b â€“ Shared Network with Actor and Critic Heads + Loss Function\n",
        "\n",
        "Now define a class `SharedActorCritic`:\n",
        "- Build a shared base network (e.g., linear layer + ReLU)\n",
        "- Create two heads: one for actor (output action probabilities) and one for critic (output state value)\n",
        "\n",
        "```python\n",
        "# TODO: Define SharedActorCritic class\n",
        "```\n",
        "\n",
        "Then:\n",
        "1. Pass a dummy input tensor through the model to obtain action probabilities and value.\n",
        "2. Simulate dummy rewards and compute advantage.\n",
        "3. Compute the actor and critic losses, combine them, and backpropagate.\n",
        "\n",
        "```python\n",
        "# TODO: Simulate shared network loss computation and backpropagation\n",
        "```\n",
        "\n",
        " Use `nn.Softmax` for actor output and `nn.Linear` for scalar critic output.\n",
        "\n",
        "ðŸ”— More reading:\n",
        "- Policy Gradient Methods: https://spinningup.openai.com/en/latest/algorithms/vpg.html\n",
        "- Actor-Critic Overview: https://www.tensorflow.org/agents/tutorials/6_reinforce_tutorial\n",
        "- PyTorch Categorical Distribution: https://pytorch.org/docs/stable/distributions.html#categorical\n",
        "\n",
        "---"
      ],
      "id": "64081a606b93029d"
    },
    {
      "metadata": {
        "id": "a48f882fff11aecc"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "\n",
        "class SharedActorCritic(nn.Module):\n",
        "  def __init__(self, input_size, output_size, hidden_size = 128):\n",
        "    super(SharedActorCritic, self).__init__()\n",
        "\n",
        "    self.shared_fc = nn.Linear(input_size, hidden_size)\n",
        "    self.shared_relu = nn.ReLU()\n",
        "\n",
        "    self.actor_head = nn.Linear(hidden_size, output_size)\n",
        "    self.critic_head = nn.Linear(hidden_size, 1)\n",
        "    self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "  def forward(self, x):\n",
        "    shared = self.shared_relu(self.shared_fc(x))\n",
        "    actor_out = self.softmax(self.actor_head(shared))\n",
        "    critic_out = self.critic_head(shared)\n",
        "\n",
        "    return actor_out, critic_out\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "a48f882fff11aecc",
      "outputs": [],
      "execution_count": 8
    },
    {
      "cell_type": "code",
      "source": [
        "batch_size = 7\n",
        "input_size = 8\n",
        "output_size = 2\n",
        "\n",
        "model = SharedActorCritic(input_size, output_size)\n",
        "obs = torch.randn(batch_size, input_size)\n",
        "\n",
        "# Forward pass\n",
        "actor_out, critic_out = model(obs)\n",
        "\n",
        "returns = torch.randn(batch_size)\n",
        "advantage = returns - critic_out.detach().squeeze()\n",
        "\n",
        "# losses\n",
        "log_probs = torch.log(actor_out)\n",
        "actor_loss = -(log_probs.mean(dim=1) * advantage).mean()\n",
        "critic_loss = F.mse_loss(critic_out.squeeze(), returns)\n",
        "\n",
        "total_loss = actor_loss + critic_loss"
      ],
      "metadata": {
        "id": "zaKA4RvJRX9J"
      },
      "id": "zaKA4RvJRX9J",
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "code",
      "source": [
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "total_loss.backward()\n",
        "optimizer.step()\n",
        "\n",
        "print(\"Actor output (probabilities):\")\n",
        "print(actor_out)\n",
        "print(\"\\nCritic output:\")\n",
        "print(critic_out)\n",
        "print(f\"\\nActor loss:  {actor_loss.item():.3f}\")\n",
        "print(f\"Critic loss: {critic_loss.item():.3f}\")\n",
        "print(f\"Total loss:  {total_loss.item():.3f}\")"
      ],
      "metadata": {
        "id": "AJ84YQZ0RXy8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "963edf26-a380-4b53-9523-85ef11048207"
      },
      "id": "AJ84YQZ0RXy8",
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Actor output (probabilities):\n",
            "tensor([[0.4978, 0.5022],\n",
            "        [0.3885, 0.6115],\n",
            "        [0.4719, 0.5281],\n",
            "        [0.6003, 0.3997],\n",
            "        [0.5541, 0.4459],\n",
            "        [0.5080, 0.4920],\n",
            "        [0.6093, 0.3907]], grad_fn=<SoftmaxBackward0>)\n",
            "\n",
            "Critic output:\n",
            "tensor([[ 0.1806],\n",
            "        [-0.2898],\n",
            "        [ 0.0814],\n",
            "        [ 0.1792],\n",
            "        [ 0.0350],\n",
            "        [-0.0008],\n",
            "        [ 0.2132]], grad_fn=<AddmmBackward0>)\n",
            "\n",
            "Actor loss:  0.082\n",
            "Critic loss: 0.322\n",
            "Total loss:  0.404\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "a974e302d1fdb028"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "\n",
        "In this `SharedActorCritic` setup, the actor and critic share the base layers, and have seperate heads for output. This sharing setup may help reduce the total number of learnable paramaters, improving the efficiency, and reducing the training time.\n",
        "\n"
      ],
      "id": "a974e302d1fdb028"
    },
    {
      "cell_type": "code",
      "source": [],
      "metadata": {
        "id": "Aw_2dh-3y1CL"
      },
      "id": "Aw_2dh-3y1CL",
      "execution_count": null,
      "outputs": []
    },
    {
      "metadata": {
        "id": "eb645eb009b85b1c"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 2: Auto-Adaptive Network Setup for Environments\n",
        "\n",
        "You will now create a function that builds a shared actor-critic network that adapts to any Gymnasium environment. This function should inspect the environment and build input/output layers accordingly."
      ],
      "id": "eb645eb009b85b1c"
    },
    {
      "metadata": {
        "id": "4223b6ddf43abee5"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 2: Auto-generate Input and Output Layers\n",
        "Write a function `create_shared_network(env)` that constructs a neural network using the following rules:\n",
        "- The input layer should match the environment's observation space.\n",
        "- The output layer for the **actor** should depend on the action space:\n",
        "  - For discrete actions: output probabilities using `nn.Softmax`.\n",
        "  - For continuous actions: output mean and log std for a Gaussian distribution.\n",
        "- The **critic** always outputs a single scalar value.\n",
        "\n",
        "```python\n",
        "# TODO: Define function `create_shared_network(env)`\n",
        "```\n",
        "\n",
        "#### Environments to Support:\n",
        "Test your function with the following environments:\n",
        "1. `CliffWalking-v0` (Use one-hot encoding for discrete integer observations.)\n",
        "2. `LunarLander-v3` (Standard Box space for observations and discrete actions.)\n",
        "3. `PongNoFrameskip-v4` (Use gym wrappers for Atari image preprocessing.)\n",
        "4. `HalfCheetah-v5` (Continuous observation and continuous action.)\n",
        "\n",
        "```python\n",
        "# TODO: Loop through environments and test `create_shared_network`\n",
        "```\n",
        "\n",
        "Hint: Use `gym.spaces` utilities to determine observation/action types dynamically.\n",
        "\n",
        "ðŸ”— Observation/Action Space Docs:\n",
        "- https://gymnasium.farama.org/api/spaces/\n",
        "\n",
        "---"
      ],
      "id": "4223b6ddf43abee5"
    },
    {
      "metadata": {
        "id": "d6d249ff9277403a"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "\n",
        "def create_shared_network(env):\n",
        "  observ_space = env.observation_space\n",
        "  action_space = env.action_space\n",
        "\n",
        "  if isinstance(observ_space, gym.spaces.Discrete):\n",
        "    input_size = observ_space.n\n",
        "    one_hot_encode = True\n",
        "  else:\n",
        "    input_size = int(np.prod(observ_space.shape))\n",
        "    one_hot_encode = False\n",
        "\n",
        "  if isinstance(action_space, gym.spaces.Discrete):\n",
        "    output_size = action_space.n\n",
        "    continuous = False\n",
        "  else:\n",
        "    output_size = action_space.shape[0]\n",
        "    continuous = True\n",
        "\n",
        "  class SharedActorCritic(nn.Module):\n",
        "    def __init__(self, input_size, output_size, hidden_size = 128):\n",
        "      super(SharedActorCritic, self).__init__()\n",
        "      self.input_size = input_size\n",
        "      self.output_size = output_size\n",
        "      self.one_hot_encode = one_hot_encode\n",
        "      self.continuous = continuous\n",
        "\n",
        "      # Shared layesr\n",
        "      self.shared_fc = nn.Linear(input_size, hidden_size)\n",
        "      self.shared_relu = nn.ReLU()\n",
        "\n",
        "      # actor\n",
        "      if self.continuous:\n",
        "        self.mean_head = nn.Linear(hidden_size, output_size)\n",
        "        self.log_std = nn.Parameter(torch.zeros(output_size))\n",
        "      else:\n",
        "        self.actor_head = nn.Linear(hidden_size, output_size)\n",
        "        self.softmax = nn.Softmax(dim=-1)\n",
        "\n",
        "      # critic\n",
        "      self.critic_head = nn.Linear(hidden_size, 1)\n",
        "\n",
        "    def forward(self, x):\n",
        "\n",
        "      if self.one_hot_encode:\n",
        "        if x.dim() > 1:\n",
        "          x = x.squeeze(-1)\n",
        "        x = F.one_hot(x.long(), num_classes=self.input_size).float()\n",
        "      else:\n",
        "        x = x.view(x.size(0), -1)\n",
        "\n",
        "      # shared\n",
        "      shared = self.shared_relu(self.shared_fc(x))\n",
        "\n",
        "      # actor\n",
        "      if self.continuous:\n",
        "        mean = self.mean_head(shared)\n",
        "        log_std = self.log_std.expand_as(mean)\n",
        "        actor_out = (mean, log_std)\n",
        "      else:\n",
        "        actor_out = self.softmax(self.actor_head(shared))\n",
        "\n",
        "      # critic\n",
        "      critic_out = self.critic_head(shared)\n",
        "\n",
        "      return actor_out, critic_out\n",
        "\n",
        "  return SharedActorCritic(input_size, output_size)\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "d6d249ff9277403a",
      "outputs": [],
      "execution_count": 11
    },
    {
      "cell_type": "code",
      "source": [
        "print(gym.make(\"CliffWalking-v1\").observation_space)\n",
        "print(gym.make(\"LunarLander-v3\").observation_space)\n",
        "print(gym.make(\"PongNoFrameskip-v4\").observation_space)\n",
        "print(gym.make(\"HalfCheetah-v5\").observation_space)"
      ],
      "metadata": {
        "id": "CTFyJSaHPUgq",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "cc6db475-db97-4857-83a7-3e93756fbf39"
      },
      "id": "CTFyJSaHPUgq",
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Discrete(48)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stderr",
          "text": [
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyPacked has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type SwigPyObject has no __module__ attribute\n",
            "<frozen importlib._bootstrap>:488: DeprecationWarning: builtin type swigvarlink has no __module__ attribute\n",
            "/usr/local/lib/python3.12/dist-packages/pygame/pkgdata.py:25: DeprecationWarning: pkg_resources is deprecated as an API. See https://setuptools.pypa.io/en/latest/pkg_resources.html\n",
            "  from pkg_resources import resource_stream, resource_exists\n",
            "/usr/local/lib/python3.12/dist-packages/jupyter_client/session.py:203: DeprecationWarning: datetime.datetime.utcnow() is deprecated and scheduled for removal in a future version. Use timezone-aware objects to represent datetimes in UTC: datetime.datetime.now(datetime.UTC).\n",
            "  return datetime.utcnow().replace(tzinfo=utc)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('google.cloud')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n",
            "/usr/local/lib/python3.12/dist-packages/pkg_resources/__init__.py:3154: DeprecationWarning: Deprecated call to `pkg_resources.declare_namespace('sphinxcontrib')`.\n",
            "Implementing implicit namespace packages (as specified in PEP 420) is preferred to `pkg_resources.declare_namespace`. See https://setuptools.pypa.io/en/latest/references/keywords.html#keyword-namespace-packages\n",
            "  declare_namespace(pkg)\n"
          ]
        },
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Box([ -2.5        -2.5       -10.        -10.         -6.2831855 -10.\n",
            "  -0.         -0.       ], [ 2.5        2.5       10.        10.         6.2831855 10.\n",
            "  1.         1.       ], (8,), float32)\n",
            "Box(0, 255, (210, 160, 3), uint8)\n",
            "Box(-inf, inf, (17,), float64)\n"
          ]
        }
      ]
    },
    {
      "cell_type": "code",
      "source": [
        "from gymnasium.wrappers import AtariPreprocessing\n",
        "\n",
        "env_names = [\"CliffWalking-v1\", \"LunarLander-v3\", \"HalfCheetah-v5\", \"PongNoFrameskip-v4\"]\n",
        "\n",
        "for name in env_names:\n",
        "    print(f\"\\n\\nTesting the {name} Environment\")\n",
        "\n",
        "    if name == \"PongNoFrameskip-v4\":\n",
        "        env = gym.make(name, frameskip=1)\n",
        "        env = AtariPreprocessing(env, grayscale_obs=True, frame_skip=1, scale_obs=False)\n",
        "    else:\n",
        "        env = gym.make(name)\n",
        "\n",
        "    network = create_shared_network(env)\n",
        "\n",
        "    state, info = env.reset()\n",
        "    state_tensor = torch.tensor(state).unsqueeze(0).float()\n",
        "\n",
        "    actor_out, critic_out = network(state_tensor)\n",
        "\n",
        "    print(\"Actor Output:\", actor_out)\n",
        "    print(\"Critic Output:\", critic_out)\n",
        "    print(network)"
      ],
      "metadata": {
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "id": "E2MMNqBByL1r",
        "outputId": "ce75c910-4fea-4ca7-b7c9-876261cb41b1"
      },
      "id": "E2MMNqBByL1r",
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "\n",
            "\n",
            "Testing the CliffWalking-v1 Environment\n",
            "Actor Output: tensor([[0.2573, 0.2549, 0.2178, 0.2700]], grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output: tensor([[-0.1218]], grad_fn=<AddmmBackward0>)\n",
            "SharedActorCritic(\n",
            "  (shared_fc): Linear(in_features=48, out_features=128, bias=True)\n",
            "  (shared_relu): ReLU()\n",
            "  (actor_head): Linear(in_features=128, out_features=4, bias=True)\n",
            "  (softmax): Softmax(dim=-1)\n",
            "  (critic_head): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "Testing the LunarLander-v3 Environment\n",
            "Actor Output: tensor([[0.1942, 0.2898, 0.2622, 0.2538]], grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output: tensor([[-0.0122]], grad_fn=<AddmmBackward0>)\n",
            "SharedActorCritic(\n",
            "  (shared_fc): Linear(in_features=8, out_features=128, bias=True)\n",
            "  (shared_relu): ReLU()\n",
            "  (actor_head): Linear(in_features=128, out_features=4, bias=True)\n",
            "  (softmax): Softmax(dim=-1)\n",
            "  (critic_head): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "Testing the HalfCheetah-v5 Environment\n",
            "Actor Output: (tensor([[ 0.0629,  0.0147,  0.0198, -0.1308, -0.0193,  0.1486]],\n",
            "       grad_fn=<AddmmBackward0>), tensor([[0., 0., 0., 0., 0., 0.]], grad_fn=<ExpandBackward0>))\n",
            "Critic Output: tensor([[-0.0795]], grad_fn=<AddmmBackward0>)\n",
            "SharedActorCritic(\n",
            "  (shared_fc): Linear(in_features=17, out_features=128, bias=True)\n",
            "  (shared_relu): ReLU()\n",
            "  (mean_head): Linear(in_features=128, out_features=6, bias=True)\n",
            "  (critic_head): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n",
            "\n",
            "\n",
            "Testing the PongNoFrameskip-v4 Environment\n",
            "Actor Output: tensor([[6.7906e-37, 4.7455e-19, 1.0000e+00, 1.0068e-34, 2.6878e-11, 5.3945e-31]],\n",
            "       grad_fn=<SoftmaxBackward0>)\n",
            "Critic Output: tensor([[11.1424]], grad_fn=<AddmmBackward0>)\n",
            "SharedActorCritic(\n",
            "  (shared_fc): Linear(in_features=7056, out_features=128, bias=True)\n",
            "  (shared_relu): ReLU()\n",
            "  (actor_head): Linear(in_features=128, out_features=6, bias=True)\n",
            "  (softmax): Softmax(dim=-1)\n",
            "  (critic_head): Linear(in_features=128, out_features=1, bias=True)\n",
            ")\n"
          ]
        }
      ]
    },
    {
      "metadata": {
        "id": "4ccd13f0b62b30ff"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "\n",
        "This code builds up a flexible SharedActorCritic network that automatically adjusts to whatever observation and action space the environment has. It builds one shared model and then splits it into an actor part and a critic part. The function returns a single network that works for any environment without needing to manually change the architecture.\n"
      ],
      "id": "4ccd13f0b62b30ff"
    },
    {
      "metadata": {
        "id": "ee2dd81024ce246a"
      },
      "cell_type": "code",
      "source": [],
      "id": "ee2dd81024ce246a",
      "outputs": [],
      "execution_count": 14
    },
    {
      "metadata": {
        "id": "b39c886fa536a639"
      },
      "cell_type": "markdown",
      "source": [
        "### Task 3: Write Observation Normalization Function\n",
        "Create a function `normalize_observation(obs, env)` that:\n",
        "- Checks if the observation space is `Box` and has `low` and `high` attributes.\n",
        "- If so, normalize the input observation.\n",
        "- Otherwise, return the observation unchanged.\n",
        "\n",
        "```python\n",
        "# TODO: Define `normalize_observation(obs, env)`\n",
        "```\n",
        "\n",
        "Test this function with observations from:\n",
        "- `LunarLander-v3`\n",
        "- `PongNoFrameskip-v4`\n",
        "\n",
        "Note: Atari observations are image arrays. Normalize pixel values to [0, 1]. For LunarLander-v3, the different elements in the observation vector have different ranges. Normalize them to [0, 1] using the `low` and `high` attributes of the observation space.\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "b39c886fa536a639"
    },
    {
      "cell_type": "code",
      "source": [
        "gym.register_envs(ale_py)\n",
        "\n",
        "from gymnasium.envs.registration import registry\n",
        "pong_envs = [env_id for env_id in registry if \"Pong\" in env_id]\n",
        "print(\"Available Pong environments:\", pong_envs)"
      ],
      "metadata": {
        "id": "v8WLPdwUgX3h",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "bf8785bc-9004-41e4-e67f-682b46af152d"
      },
      "id": "v8WLPdwUgX3h",
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Available Pong environments: ['Pong-v0', 'Pong-v4', 'PongNoFrameskip-v0', 'PongNoFrameskip-v4', 'ALE/Pong-v5']\n"
          ]
        }
      ]
    },
    {
      "cell_type": "markdown",
      "source": [],
      "metadata": {
        "id": "diyLdF3xgUM2"
      },
      "id": "diyLdF3xgUM2"
    },
    {
      "metadata": {
        "id": "fc7ee06112cf7d29",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5beaa0bf-dcae-4bb1-f43c-046843e2744e"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "\n",
        "def normalize_observation(obs, env):\n",
        "    obs_space = env.observation_space\n",
        "\n",
        "    if not isinstance(obs_space, gym.spaces.Box):\n",
        "        return obs\n",
        "\n",
        "    is_tensor = isinstance(obs, torch.Tensor)\n",
        "    if is_tensor:\n",
        "        obs_np = obs.detach().cpu().numpy().astype(np.float32)\n",
        "    else:\n",
        "        obs_np = np.array(obs, dtype=np.float32)\n",
        "\n",
        "    low = obs_space.low\n",
        "    high = obs_space.high\n",
        "\n",
        "    if not (np.all(np.isfinite(low)) and np.all(np.isfinite(high))):\n",
        "        return obs\n",
        "\n",
        "    is_pixel_space = (\n",
        "        np.issubdtype(obs_space.dtype, np.integer)\n",
        "        or (low.min() == 0 and high.max() == 255)\n",
        "    )\n",
        "    if is_pixel_space:\n",
        "        norm = obs_np / 255.0\n",
        "    else:\n",
        "        denom = (high - low)\n",
        "        denom[denom == 0] = 1.0\n",
        "        norm = 2.0 * (obs_np - low) / denom - 1.0\n",
        "\n",
        "    if is_tensor:\n",
        "        norm_tensor = torch.from_numpy(norm).to(dtype=obs.dtype, device=obs.device)\n",
        "        return norm_tensor\n",
        "    else:\n",
        "        return norm.astype(np.float32)\n",
        "\n",
        "# LunarLander-v3\n",
        "ll_env = gym.make(\"LunarLander-v3\")\n",
        "ll_obs, ll_info = ll_env.reset()\n",
        "ll_obs_norm = normalize_observation(ll_obs, ll_env)\n",
        "print(\"LunarLander-v3:\")\n",
        "print(\"  raw obs:        \", ll_obs)\n",
        "print(\"  normalized obs: \", ll_obs_norm)\n",
        "print(\"  normalized range: [\", ll_obs_norm.min(), \",\", ll_obs_norm.max(), \"]\\n\")\n",
        "\n",
        "# PongNoFrameskip-v4\n",
        "pong_env = gym.make(\"PongNoFrameskip-v4\")\n",
        "pong_obs, pong_info = pong_env.reset()\n",
        "\n",
        "print(f\"PongNoFrameskip-v4:\")\n",
        "print(f\"Raw observation shape:  {pong_obs.shape}\")\n",
        "print(f\"Raw observation dtype:  {pong_obs.dtype}\")\n",
        "print(f\"Raw observation range:  [{pong_obs.min()}, {pong_obs.max()}]\")\n",
        "print(f\"Sample pixel at [100,80]: {pong_obs[100, 80]}\")\n",
        "\n",
        "pong_obs_norm = normalize_observation(pong_obs, pong_env)\n",
        "\n",
        "print(f\"\\nNormalized shape:       {pong_obs_norm.shape}\")\n",
        "print(f\"Normalized dtype:       {pong_obs_norm.dtype}\")\n",
        "print(f\"Normalized range:       [{pong_obs_norm.min():.3f}, {pong_obs_norm.max():.3f}]\")\n",
        "print(f\"Sample normalized pixel at [100,80]: {pong_obs_norm[100, 80]}\")\n",
        "print(f\"Observation space:      {pong_env.observation_space}\")\n",
        "\n",
        "pong_env.close()\n",
        "\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "fc7ee06112cf7d29",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "LunarLander-v3:\n",
            "  raw obs:         [-7.0590974e-04  1.4071814e+00 -7.1529314e-02 -1.6616482e-01\n",
            "  8.2488253e-04  1.6202472e-02  0.0000000e+00  0.0000000e+00]\n",
            "  normalized obs:  [-2.8240681e-04  5.6287253e-01 -7.1529150e-03 -1.6616464e-02\n",
            "  1.3124943e-04  1.6202927e-03 -1.0000000e+00 -1.0000000e+00]\n",
            "  normalized range: [ -1.0 , 0.5628725 ]\n",
            "\n",
            "PongNoFrameskip-v4:\n",
            "Raw observation shape:  (210, 160, 3)\n",
            "Raw observation dtype:  uint8\n",
            "Raw observation range:  [0, 228]\n",
            "Sample pixel at [100,80]: [109 118  43]\n",
            "\n",
            "Normalized shape:       (210, 160, 3)\n",
            "Normalized dtype:       float32\n",
            "Normalized range:       [0.000, 0.894]\n",
            "Sample normalized pixel at [100,80]: [0.42745098 0.4627451  0.16862746]\n",
            "Observation space:      Box(0, 255, (210, 160, 3), uint8)\n"
          ]
        }
      ],
      "execution_count": 16
    },
    {
      "metadata": {
        "id": "501ed2a6e7ca7a7b"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "\n",
        "The normalization function is built to handle two common types of observations in reinforcement learning: low-dimensional state vectors and high-dimensional image observations. It first checks whether the environmentâ€™s observation space is a Box with finite lower and upper bounds. If the space is not a Box or has infinite bounds, the function simply returns the original observation. This avoids making unsafe assumptions about unbounded or non-numeric observations and keeps the preprocessing focused on cases where scaling is well defined.\n",
        "\n",
        "When the observation comes from a vector-valued Box space, such as in LunarLander-v3, the function uses the environmentâ€™s low and high arrays to normalize each dimension into the range [âˆ’1, 1]. This is useful because different state variables (position, velocity, angle, leg contacts, etc.) can naturally live on very different scales. Mapping all of them into a common range helps neural networks train more smoothly: gradients are less dominated by any single large-scale feature, and learning tends to be more stable. This setup is preferred for classic control and continuous-state environments where the state is a relatively small numeric vector with meaningful physical bounds.\n",
        "\n",
        "For pixel-based environments like PongNoFrameskip-v4, the observation space is a Box(0, 255, (210, 160, 3), uint8), representing RGB images. In this case, the function detects an integer Box with 0â€“255 bounds and normalizes by dividing by 255, which maps pixel values into the range [0, 1] while preserving the image shape. This is a standard preprocessing step in vision tasks and makes the input more suitable for gradient-based optimization, since working with large raw integers can slow down or destabilize training. This setup is preferred whenever the agent receives raw images from the environment, especially in Atari-style games.\n",
        "\n",
        "Overall, the two setups: [âˆ’1, 1] scaling for vector states and [0, 1] scaling for pixel observationsâ€”provide a simple and practical normalization strategy that aligns with how these different types of inputs are usually handled in practice. The same function can be reused across both types of environments, reducing code duplication and making it easier to plug different tasks into the same training pipeline."
      ],
      "id": "501ed2a6e7ca7a7b"
    },
    {
      "metadata": {
        "id": "78211b617a843f62"
      },
      "cell_type": "code",
      "source": [],
      "id": "78211b617a843f62",
      "outputs": [],
      "execution_count": 16
    },
    {
      "metadata": {
        "id": "6b5fb5353307f514"
      },
      "cell_type": "markdown",
      "source": [
        "## Section 4: Gradient Clipping\n",
        "\n",
        "To prevent exploding gradients, it's common practice to clip gradients before optimizer updates.\n",
        "\n",
        "### Task 4: Clip Gradients for Actor-Critic Networks\n",
        "Use dummy tensors and apply gradient clipping with the following PyTorch method:\n",
        "```python\n",
        "# During training, after loss.backward():\n",
        "torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5)\n",
        "```\n",
        "\n",
        "Reuse the loss computation from Task 1a or 1b. After computing the gradients, apply gradient clipping.\n",
        "Print the gradient norm before and after clipping to verify itâ€™s applied.\n",
        "\n",
        "ðŸ”— PyTorch Docs: https://pytorch.org/docs/stable/generated/torch.nn.utils.clip_grad_norm_.html\n",
        "\n",
        "\n",
        "---"
      ],
      "id": "6b5fb5353307f514"
    },
    {
      "metadata": {
        "id": "7327507fb6e803ad",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "90033025-86e0-45e8-baa5-0978b7cce9aa"
      },
      "cell_type": "code",
      "source": [
        "# BEGIN_YOUR_CODE\n",
        "\n",
        "batch_size = 7\n",
        "input_size = 8\n",
        "output_size = 2\n",
        "model = SharedActorCritic(input_size, output_size)\n",
        "\n",
        "obs = torch.randn(batch_size, input_size)\n",
        "returns = torch.randn(batch_size)\n",
        "\n",
        "actor_out, critic_out = model(obs)\n",
        "\n",
        "advantage = returns - critic_out.detach().squeeze()\n",
        "log_probs = torch.log(actor_out + 1e-8)\n",
        "actor_loss = -(log_probs.mean(dim=1) * advantage).mean()\n",
        "critic_loss = F.mse_loss(critic_out.squeeze(), returns)\n",
        "total_loss = actor_loss + critic_loss\n",
        "\n",
        "optimizer = optim.Adam(model.parameters(), lr=0.001)\n",
        "\n",
        "optimizer.zero_grad()\n",
        "total_loss.backward()\n",
        "\n",
        "print(\"Single Optimizer Training Step:\")\n",
        "print(f\"Actor Loss: {actor_loss.item()}\")\n",
        "print(f\"Critic Loss: {critic_loss.item()}\")\n",
        "print(f\"Total Loss: {total_loss.item()}\")\n",
        "\n",
        "total_norm_before = 0.0\n",
        "for p in model.parameters():\n",
        "    if p.grad is not None:\n",
        "        param_norm = p.grad.data.norm(2)\n",
        "        total_norm_before += param_norm.item() ** 2\n",
        "total_norm_before = total_norm_before ** 0.5\n",
        "\n",
        "print(f\"Gradient norm before clipping: {total_norm_before}\")\n",
        "\n",
        "max_grad_norm = 0.5\n",
        "grad_norm_reported = torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=max_grad_norm)\n",
        "\n",
        "print(f\"Gradient norm reported by clip_grad_norm_ (before clipping): {grad_norm_reported}\")\n",
        "\n",
        "total_norm_after = 0.0\n",
        "for p in model.parameters():\n",
        "    if p.grad is not None:\n",
        "        param_norm = p.grad.data.norm(2)\n",
        "        total_norm_after += param_norm.item() ** 2\n",
        "total_norm_after = total_norm_after ** 0.5\n",
        "\n",
        "print(f\"Gradient norm after clipping: {total_norm_after}\")\n",
        "\n",
        "optimizer.step()\n",
        "\n",
        "# END_YOUR_CODE"
      ],
      "id": "7327507fb6e803ad",
      "outputs": [
        {
          "output_type": "stream",
          "name": "stdout",
          "text": [
            "Single Optimizer Training Step:\n",
            "Actor Loss: 0.05454203858971596\n",
            "Critic Loss: 0.69195556640625\n",
            "Total Loss: 0.746497631072998\n",
            "Gradient norm before clipping: 3.2099005899061894\n",
            "Gradient norm reported by clip_grad_norm_ (before clipping): 3.2099006175994873\n",
            "Gradient norm after clipping: 0.49999987145309605\n"
          ]
        }
      ],
      "execution_count": 17
    },
    {
      "metadata": {
        "id": "9952750fa74cd487"
      },
      "cell_type": "markdown",
      "source": [
        "### Discuss the motivation behind each setup and when it may be preferred in practice.\n",
        "\n",
        "YOUR ANSWER:\n",
        "\n",
        "\n",
        "The code uses a shared SharedActorCritic network with one optimizer step to show how gradient clipping works. The network takes an 8-dimensional observation, passes it through shared layers, and then splits into two heads: one head outputs action probabilities (actor), and the other predicts a value estimate (critic). The actor loss is computed using a policy-gradient style term with an advantage (return - value), and the critic loss is the mean-squared error between the predicted values and the target returns. These two losses are added into a single total loss, and total_loss.backward() is called so that gradients are computed for all parts of the model.\n",
        "\n",
        "After backpropagation, the code calculates the global L2 norm of all gradients before clipping, applies torch.nn.utils.clip_grad_norm_(model.parameters(), max_norm=0.5), and then measures the gradient norm again. In the example run, the gradient norm starts at about 3.40 and is reduced to around 0.5 after clipping. This shows that gradient clipping is actually limiting the size of the update. This kind of setupâ€”a shared actorâ€“critic network with a single optimizer and gradient norm clippingâ€”is useful in reinforcement learning when gradients can suddenly become large due to noisy returns or advantages. Clipping keeps the training step under control and helps prevent unstable updates, while still allowing the model to learn from both the actor and critic losses at the same time.\n",
        "\n"
      ],
      "id": "9952750fa74cd487"
    },
    {
      "metadata": {
        "id": "557a9303f5a1c863"
      },
      "cell_type": "code",
      "source": [],
      "id": "557a9303f5a1c863",
      "outputs": [],
      "execution_count": 17
    },
    {
      "metadata": {
        "id": "f4cff31e6c6e7e4a"
      },
      "cell_type": "markdown",
      "source": [
        "If you are working in a team, provide a contribution summary.\n",
        "| Team Member | Step# | Contribution (%) |\n",
        "|---|---|---|\n",
        "| Ziyad Shahin, Jahnavi Gubbala\t  | Task 1 |  100% |\n",
        "| Ziyad Shahin, Jahnavi Gubbala\t  | Task 2 |  100%  |\n",
        "|  Ziyad Shahin, Jahnavi Gubbala\t| Task 3 |  100% |\n",
        "|  Ziyad Shahin, Jahnavi Gubbala\t| Task 4 |  100% |\n",
        "|   | **Total** |   |\n"
      ],
      "id": "f4cff31e6c6e7e4a"
    }
  ],
  "metadata": {
    "kernelspec": {
      "display_name": "Python 3",
      "language": "python",
      "name": "python3"
    },
    "language_info": {
      "codemirror_mode": {
        "name": "ipython",
        "version": 2
      },
      "file_extension": ".py",
      "mimetype": "text/x-python",
      "name": "python",
      "nbconvert_exporter": "python",
      "pygments_lexer": "ipython2",
      "version": "2.7.6"
    },
    "colab": {
      "provenance": [],
      "include_colab_link": true
    }
  },
  "nbformat": 4,
  "nbformat_minor": 5
}